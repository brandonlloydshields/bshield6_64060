---
title: "bshield6_Assignment_3"
author: "Brandon Lloyd Shields"
date: "11/14/2021"
output:
  word_document: default
  html_document: default
---

```{r Load Libraries, cache=TRUE}
library(keras)
library (dplyr)
```

```{r Getting Database}

#Importing dataset from web download. 

imdb_dir <- "~/Downloads/aclImdb"
train_dir <- file.path(imdb_dir, "train")

labels <- c()
texts <- c()

#Creating Labeled dataset for traibning and validating

for (label_type in c("neg", "pos")) {
  label <- switch(label_type, neg = 0, pos = 1)
  dir_name <- file.path(train_dir, label_type)
  for (fname in list.files(dir_name, pattern = glob2rx("*.txt"), 
                           full.names = TRUE)) {
    texts <- c(texts, readChar(fname, file.info(fname)$size))
    labels <- c(labels, label)
  }
}
```

```{r Data Preprocessing, cache=TRUE}

#Setting parameters based on assignment instructions.

maxlen <- 150                 # We will cut reviews after 150 words
training_samples <- 100       # We will be training on 100 samples
validation_samples <- 10000   # We will be validating on 10000 samples
max_words <- 10000            # We will only consider the top 10,000 words in the dataset

tokenizer <- text_tokenizer(num_words = max_words) %>% 
  fit_text_tokenizer(texts)

sequences <- texts_to_sequences(tokenizer, texts)

word_index = tokenizer$word_index
cat("Found", length(word_index), "unique tokens.\n")

data <- pad_sequences(sequences, maxlen = maxlen)

labels <- as.array(labels)
cat("Shape of data tensor:", dim(data), "\n")
cat('Shape of label tensor:', dim(labels), "\n")

# Split the data into a training set and a validation set and shuffeling the data
# since samples are ordered

indices <- sample(1:nrow(data))
training_indices <- indices[1:training_samples]
validation_indices <- indices[(training_samples + 1): 
                              (training_samples + validation_samples)]

x_train <- data[training_indices,]
y_train <- labels[training_indices]

x_val <- data[validation_indices,]
y_val <- labels[validation_indices]
```

```{r Pre-Trained Kayers, cache = TRUE}

#activating pre-trained layers to include in model

glove_dir = '~/Downloads/glove'
lines <- readLines(file.path(glove_dir, "glove.6B.100d.txt"))

embeddings_index <- new.env(hash = TRUE, parent = emptyenv())
for (i in 1:length(lines)) {
  line <- lines[[i]]
  values <- strsplit(line, " ")[[1]]
  word <- values[[1]]
  embeddings_index[[word]] <- as.double(values[-1])
}

cat("Found", length(embeddings_index), "word vectors.\n")
```

```{r Building Embedding Matrix, cache= TRUE}

embedding_dim <- 100

embedding_matrix <- array(0, c(max_words, embedding_dim))

for (word in names(word_index)) {
  index <- word_index[[word]]
  if (index < max_words) {
    embedding_vector <- embeddings_index[[word]]
    if (!is.null(embedding_vector))
      # Words not found in the embedding index will be all zeros.
      embedding_matrix[index+1,] <- embedding_vector
  }
}
```

```{r Model Pre-Trained 100 training samples}

model <- keras_model_sequential() %>% 
  layer_embedding(input_dim = max_words, output_dim = embedding_dim, 
                  input_length = maxlen) %>% 
  layer_flatten() %>% 
  layer_dense(units = 32, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")
```

```{r Get Pre-Trained Layer}
get_layer(model, index = 1) %>% 
  set_weights(list(embedding_matrix)) %>% 
  freeze_weights()
```

```{r Model Evaluation, cache = TRUE}

model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("acc")
)

history.pt.100 <- model %>% fit(
  x_train, y_train,
  epochs = 20,
  batch_size = 32,
  validation_data = list(x_val, y_val)
)

save_model_weights_hdf5(model, "pre_trained_glove_model.h5")

```

```{r cache = TRUE}
plot(history.pt.100)
```

```{r Model Adjustments for 500 PT 500 samples, cache = TRUE}

#Crearibg various training Samples Sizes for all future models
training_samples_500 <- 500
training_samples_1000 <- 1000
training_samples_1500 <- 1500
training_samples_2000 <- 2000

#Indicies with 500 training set
indices_500 <- sample(1:nrow(data))
training_indices_500 <- indices[1:training_samples_500]
validation_indices_500 <- indices[(training_samples_500 + 1): 
                              (training_samples_500 + validation_samples)]

x_train_500 <- data[training_indices_500,]
y_train_500 <- labels[training_indices_500]

x_val_500 <- data[validation_indices_500,]
y_val_500 <- labels[validation_indices_500]

model.pt.500 <- keras_model_sequential() %>% 
  layer_embedding(input_dim = max_words, output_dim = embedding_dim, 
                  input_length = maxlen) %>% 
  layer_flatten() %>% 
  layer_dense(units = 32, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")

get_layer(model.pt.500, index = 1) %>% 
  set_weights(list(embedding_matrix)) %>% 
  freeze_weights()

model.pt.500 %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("acc")
)

history.pt.500 <- model %>% fit(
  x_train_500, y_train_500,
  epochs = 20,
  batch_size = 32,
  validation_data = list(x_val_500, y_val_500)
)

save_model_weights_hdf5(model, "pre_trained_glove_model.h5_pt500")


plot(history.pt.500)

```


```{r Pretrained Model with 1000 samples, cache=TRUE}
#Indicies with 1000 training set

indices_1000 <- sample(1:nrow(data))
training_indices_1000 <- indices[1:training_samples_1000]
validation_indices_1000 <- indices[(training_samples_1000 + 1): 
                              (training_samples_1000 + validation_samples)]

x_train_1000 <- data[training_indices_1000,]
y_train_1000 <- labels[training_indices_1000]

x_val_1000 <- data[validation_indices_1000,]
y_val_1000 <- labels[validation_indices_1000]

model.pt.1000 <- keras_model_sequential() %>% 
  layer_embedding(input_dim = max_words, output_dim = embedding_dim, 
                  input_length = maxlen) %>% 
  layer_flatten() %>% 
  layer_dense(units = 32, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")

get_layer(model.pt.1000, index = 1) %>% 
  set_weights(list(embedding_matrix)) %>% 
  freeze_weights()

model.pt.1000 %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("acc")
)

history.pt.1000 <- model %>% fit(
  x_train_1000, y_train_1000,
  epochs = 20,
  batch_size = 32,
  validation_data = list(x_val_1000, y_val_1000)
)

save_model_weights_hdf5(model, "pre_trained_glove_model.h5_pt1000")


plot(history.pt.1000)
```


```{r Pretrained Model with 1500 samples, cache=TRUE}
#Indicies with 1500 training set

indices_1500 <- sample(1:nrow(data))
training_indices_1500 <- indices[1:training_samples_1500]
validation_indices_1500 <- indices[(training_samples_1500 + 1): 
                              (training_samples_1500 + validation_samples)]

x_train_1500 <- data[training_indices_1500,]
y_train_1500 <- labels[training_indices_1500]

x_val_1500 <- data[validation_indices_1500,]
y_val_1500 <- labels[validation_indices_1500]

model.pt.1500 <- keras_model_sequential() %>% 
  layer_embedding(input_dim = max_words, output_dim = embedding_dim, 
                  input_length = maxlen) %>% 
  layer_flatten() %>% 
  layer_dense(units = 32, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")

get_layer(model.pt.1500, index = 1) %>% 
  set_weights(list(embedding_matrix)) %>% 
  freeze_weights()

model.pt.1500 %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("acc")
)

history.pt.1500 <- model %>% fit(
  x_train_1500, y_train_1500,
  epochs = 20,
  batch_size = 32,
  validation_data = list(x_val_1500, y_val_1500)
)

save_model_weights_hdf5(model, "pre_trained_glove_model.h5_pt1500")


plot(history.pt.1500)
```

```{r Pretrained Model with 2000 samples, cache=TRUE}
#Indicies with 2000 training set

indices_2000 <- sample(1:nrow(data))
training_indices_2000 <- indices[1:training_samples_2000]
validation_indices_2000 <- indices[(training_samples_2000 + 1): 
                              (training_samples_2000 + validation_samples)]

x_train_2000 <- data[training_indices_2000,]
y_train_2000 <- labels[training_indices_2000]

x_val_2000 <- data[validation_indices_2000,]
y_val_2000 <- labels[validation_indices_2000]

model.pt.2000 <- keras_model_sequential() %>% 
  layer_embedding(input_dim = max_words, output_dim = embedding_dim, 
                  input_length = maxlen) %>% 
  layer_flatten() %>% 
  layer_dense(units = 32, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")

get_layer(model.pt.2000, index = 1) %>% 
  set_weights(list(embedding_matrix)) %>% 
  freeze_weights()

model.pt.2000 %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("acc")
)

history.pt.2000 <- model %>% fit(
  x_train_2000, y_train_2000,
  epochs = 20,
  batch_size = 32,
  validation_data = list(x_val_2000, y_val_2000)
)

save_model_weights_hdf5(model, "pre_trained_glove_model.h5_pt2000")


plot(history.pt.2000)
```

```{r Embedded Layer Model 100 samples, cache=TRUE}
model.100 <- keras_model_sequential() %>% 
  layer_embedding(input_dim = max_words, output_dim = embedding_dim, 
                  input_length = maxlen) %>% 
  layer_flatten() %>% 
  layer_dense(units = 32, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")

model.100 %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("acc")
)

history.100 <- model.100 %>% fit(
  x_train, y_train,
  epochs = 20,
  batch_size = 32,
  validation_data = list(x_val, y_val)
)

plot(history.100)
```

```{r Embedded Layer Model 500 samples, cache=TRUE}
model.500 <- keras_model_sequential() %>% 
  layer_embedding(input_dim = max_words, output_dim = embedding_dim, 
                  input_length = maxlen) %>% 
  layer_flatten() %>% 
  layer_dense(units = 32, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")

model.500 %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("acc")
)

history.500 <- model.500 %>% fit(
  x_train_500, y_train_500,
  epochs = 20,
  batch_size = 32,
  validation_data = list(x_val_500, y_val_500)
)

plot(history.500)
```

```{r Embedded Layer Model 1000 samples, cache=TRUE}
model.1000 <- keras_model_sequential() %>% 
  layer_embedding(input_dim = max_words, output_dim = embedding_dim, 
                  input_length = maxlen) %>% 
  layer_flatten() %>% 
  layer_dense(units = 32, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")

model.1000 %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("acc")
)

history.1000 <- model.1000 %>% fit(
  x_train_1000, y_train_1000,
  epochs = 20,
  batch_size = 32,
  validation_data = list(x_val_1000, y_val_1000)
)

plot(history.1000)
```

```{r Embedded Layer Model 2000 samples, cache=TRUE}
model.2000 <- keras_model_sequential() %>% 
  layer_embedding(input_dim = max_words, output_dim = embedding_dim, 
                  input_length = maxlen) %>% 
  layer_flatten() %>% 
  layer_dense(units = 32, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")

model.2000 %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("acc")
)

history.2000 <- model.2000 %>% fit(
  x_train_2000, y_train_2000,
  epochs = 20,
  batch_size = 32,
  validation_data = list(x_val_2000, y_val_2000)
)

plot(history.2000)
```



```{r Embedded Layer Model 150 samples, cache=TRUE}
model.1500 <- keras_model_sequential() %>% 
  layer_embedding(input_dim = max_words, output_dim = embedding_dim, 
                  input_length = maxlen) %>% 
  layer_flatten() %>% 
  layer_dense(units = 32, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")

model.1500 %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("acc")
)

history.1500 <- model.1500 %>% fit(
  x_train_1500, y_train_1500,
  epochs = 20,
  batch_size = 32,
  validation_data = list(x_val_1500, y_val_1500)
)

plot(history.1500)
```

```{r LTSN RNN Model 100 samples, cache=TRUE}
model.rnn.100 <- keras_model_sequential() %>% 
  layer_embedding(input_dim = max_words, output_dim = 32) %>% 
  layer_lstm(units = 32) %>% 
  layer_dense(units = 1, activation = "sigmoid")

model.rnn.100 %>% compile(
  optimizer = "rmsprop", 
  loss = "binary_crossentropy", 
  metrics = c("acc")
)

history.rnn.100 <- model.rnn.100 %>% fit(
  x_train, y_train,
  epochs = 20,
  batch_size = 32,
  validation_data = list(x_val, y_val)
)

plot(history.rnn.100)
```



```{r LTSN RNN Model 500 samples, cache=TRUE}
model.rnn.500 <- keras_model_sequential() %>% 
  layer_embedding(input_dim = max_words, output_dim = 32) %>% 
  layer_lstm(units = 32) %>% 
  layer_dense(units = 1, activation = "sigmoid")

model.rnn.500 %>% compile(
  optimizer = "rmsprop", 
  loss = "binary_crossentropy", 
  metrics = c("acc")
)

history.rnn.500 <- model.rnn.500 %>% fit(
  x_train_500, y_train_500,
  epochs = 20,
  batch_size = 32,
  validation_data = list(x_val_500, y_val_500)
)

plot(history.rnn.500)
```



```{r LTSN RNN Model 1000 samples, cache=TRUE}
model.rnn.1000 <- keras_model_sequential() %>% 
  layer_embedding(input_dim = max_words, output_dim = 32) %>% 
  layer_lstm(units = 32) %>% 
  layer_dense(units = 1, activation = "sigmoid")

model.rnn.1000 %>% compile(
  optimizer = "rmsprop", 
  loss = "binary_crossentropy", 
  metrics = c("acc")
)

history.rnn.1000 <- model.rnn.1000 %>% fit(
  x_train_1000, y_train_1000,
  epochs = 20,
  batch_size = 32,
  validation_data = list(x_val_1000, y_val_1000)
)

plot(history.rnn.1000)
```



```{r LTSN RNN Model 1500 samples, cache=TRUE}
model.rnn.1500 <- keras_model_sequential() %>% 
  layer_embedding(input_dim = max_words, output_dim = 32) %>% 
  layer_lstm(units = 32) %>% 
  layer_dense(units = 1, activation = "sigmoid")

model.rnn.1500 %>% compile(
  optimizer = "rmsprop", 
  loss = "binary_crossentropy", 
  metrics = c("acc")
)

history.rnn.1500 <- model.rnn.1500 %>% fit(
  x_train_1500, y_train_1500,
  epochs = 20,
  batch_size = 32,
  validation_data = list(x_val_1500, y_val_1500)
)

plot(history.rnn.1500)
```


```{r LTSN RNN Model 2000 samples, cache=TRUE}
model.rnn.2000 <- keras_model_sequential() %>% 
  layer_embedding(input_dim = max_words, output_dim = 32) %>% 
  layer_lstm(units = 32) %>% 
  layer_dense(units = 1, activation = "sigmoid")

model.rnn.2000 %>% compile(
  optimizer = "rmsprop", 
  loss = "binary_crossentropy", 
  metrics = c("acc")
)

history.rnn.2000 <- model.rnn.2000 %>% fit(
  x_train_2000, y_train_2000,
  epochs = 20,
  batch_size = 32,
  validation_data = list(x_val_2000, y_val_2000)
)

plot(history.rnn.2000)
```


```{r Creating Comparison Table and Graph}
library(dplyr)
library( tidyr)

ModeL_Type <- c("Embedded Layer", "PT Layer", "PT LSTM Layer")



"100" <- c(mean(history.100$metrics$val_acc),
                  mean(history.pt.100$metrics$val_acc), 
                  mean(history.rnn.100$metrics$val_acc))

"500" <- c(mean(history.500$metrics$val_acc),
                  mean(history.pt.500$metrics$val_acc), 
                  mean(history.rnn.500$metrics$val_acc))

"1000" <- c(mean(history.1000$metrics$val_acc),
                  mean(history.pt.1000$metrics$val_acc), 
                  mean(history.rnn.1000$metrics$val_acc))

"1500" <- c(mean(history.1500$metrics$val_acc),
                  mean(history.pt.1500$metrics$val_acc), 
                  mean(history.rnn.1500$metrics$val_acc))

"2000" <- c(mean(history.2000$metrics$val_acc),
                  mean(history.pt.2000$metrics$val_acc), 
                  mean(history.rnn.2000$metrics$val_acc))

AVG_Val_Table <- data_frame(ModeL_Type, `100`, `500`, `1000`, `1500`, `2000`)


AVG_Val_Table

AVG_Table_Tidy <- gather(AVG_Val_Table,"Sample Size", "AVG Validation Accuracy", 2:6)

AVG_Table_Tidy$`Sample Size` <- as.numeric(AVG_Table_Tidy$`Sample Size`)

AVG_Table_Tidy




```

```{r Visual Comparison}
#Creating Visual comparison

library(ggplot2)

viz <- ggplot(AVG_Table_Tidy, aes(`Sample Size`,`AVG Validation Accuracy`)) + 
  geom_line(aes(color = ModeL_Type)) + 
  ggtitle("Validation Accuracy with Varied Training Sample Saizes")

viz
```











